# Question-Answering-System-in-Telugu
~Project done by[Shirish Addaganti](https://github.com/shirish-01), [Sreehith Reddy  Gaddam](https://github.com/Sreehith128),[Ganesh Dende](https://github.com/dende-ganesh)

After thorough research across all the models of BERT, we have selected XLM-RoBERTa large trained on squad which is the best cross-lingual model compared to other models. After selecting the model, the next step is to import the model, tokenize and train the model with our custom dataset.

We have set hyperparameters like batch_size to 4, and max_length to 384. Although we have kept most hyperparameters default except for a few of them. After this, all these parameters are given as arguments to a trainer function to train the model. The trained model is saved as ‘TeluguQATrained’.Now validation prepare_validation_features function is called which is used to validate and deduce input_ids, attention_mask,offset_mapping, and example_id. Now, these valid features (input_id’s and attention_mask) are given to predict the function used to predict based on given input features. These predicted answers are in an encoded format which is then decoded and this is stored as the Final_predictions data frame. This data frame consists of predicted answers deduced by our model. Using these final predictions we have compared to original answers and metric evaluation is performed.

We have created a GUI using Streamlit, an API that is used for model deployment which is very much flexible and less time-consuming. This is a website run on a local host that is on the user system. As the website is launched the model is loaded and two input cells are given specifying context and question into cells respectively. On submitting the cells our finetuned model i.e TeluguQAtrained is run with given features as input and the output is decoded and printed back on the website to the user. Instead of loading the model every time we add the model into cash memory using decorators which reduces time complexity for the user.

This can also be developed to open domain aswell
